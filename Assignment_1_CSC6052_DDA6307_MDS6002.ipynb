{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElementQi/ElementQi/blob/main/Assignment_1_CSC6052_DDA6307_MDS6002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Assignment 1: Exploring Word Embeddings\n",
        "**Course Name:** Natural Language Processing (CSC6052/DDA6307/MDS6002)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yFFKuDZB_ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Please enter your personal information (make sure you have copied this colab)*\n",
        "\n",
        "**Name:**\n",
        "\n",
        "**Student ID:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Requirements\n",
        "\n",
        "This Colab file includes all contents for Assignment 1.\n",
        "\n",
        "#### You are required to:\n",
        "\n",
        "1. **Make a copy of the provided Google Colab file.**  \n",
        "   First, you need to make a copy of the provided file into your own Google Drive. To accomplish this, open the Colab file link, navigate to `File` → `Save a copy in Drive`.\n",
        "\n",
        "2. **Execute the notebook to generate results.**  \n",
        "   You can click on \"Connect to GPU\" to apply for a free T4 GPU. Then, you can press the large play button to run a code cell.\n",
        "\n",
        "3. **Complete the Necessary Parts.**  \n",
        "   Some sections of the code are incomplete and require your input, especially pay attention to the parts marked with **<font color=\"red\">[Task]</font>**. These sections are critical for scoring the assignment.\n",
        "\n",
        "For more detailed instructions, refer to [Working with Google Colab](https://docs.google.com/document/d/1vMe8kC-oSyP3w7rIurDbG3NqfyQw7sZJ2C_S2ngtQnk/edit?usp=sharing).\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "Follow these steps to submit your assignment:\n",
        "\n",
        "1. **Export the Notebook:** Navigate to `File` → `Download .ipynb` to download your notebook.\n",
        "\n",
        "2. **Upload Your File:** Access the [Blackboard system](https://bb.cuhk.edu.cn/) and upload your `.ipynb` file."
      ],
      "metadata": {
        "id": "nQDMk5Uu1niv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "*Assignment 1* consists of two tasks:\n",
        "- Task 1: Train Word Embeddings with Word2Vec (5 points)\n",
        "- Task 2: Utilize word embeddings (5 ponits)\n",
        "\n",
        "Your task is to **run all the code in this script** and complete the parts marked with <font color=\"red\">[task]</font>.\n",
        "\n",
        "## Prerequisite\n",
        "If you're new to Python, Numpy, or PyTorch, consider these tutorials for a quick start:\n",
        "- [Python-Numpy-Tutorial](https://cs231n.github.io/python-numpy-tutorial/)\n",
        "- [Introduction to PyTorch](https://colab.research.google.com/drive/1obAmmGHsMizB38aiZJ_-L1bVMT5KOLMd?usp=sharing)"
      ],
      "metadata": {
        "id": "H0gO12GJE06O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Train Word Embeddings with Word2Vec\n",
        "\n",
        "**In this task, you will implement and train your own Word2Vec model.**\n",
        "\n",
        "Before diving in, let's clarify what Word2Vec is.\n",
        "\n",
        "Its core concept is straightforward: you can infer the meaning of a word from its neighbors - the words that frequently appear in the same context. Consider this illustration:\n",
        "![Contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "\n",
        "A basic approach is to use the context word counts as meaningful word vectors. Take this simple corpus for example:\n",
        "\n",
        "```\n",
        "The red fox jumped\n",
        "The brown fox jumped\n",
        "```\n",
        "\n",
        "The count vectors would look like this:\n",
        "```\n",
        "        the fox jumped red brown\n",
        "red   = (1   1    1     0    0)\n",
        "brown = (1   1    1     0    0)\n",
        "```\n",
        "\n",
        "Notice how `red` and `brown` have similar vectors! We're close to solving the problem, but the goal is to obtain more compact embedding vectors.\n",
        "\n",
        "This is where Word2Vec algorithms come into play. They construct embedding vectors based on the word's neighbors in the corpus.\n",
        "\n",
        "For a more detailed introduction, check out this post: [king - man + woman = queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html).\n",
        "\n",
        "Let's do some preparation work before moving to the interesting stuff.\n",
        "\n"
      ],
      "metadata": {
        "id": "SwAJjpp-K8RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Preparation**"
      ],
      "metadata": {
        "id": "8rK5IxppLRHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment installation and data download"
      ],
      "metadata": {
        "id": "blHR2OjyO5mg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vacV4BIFI8l"
      },
      "source": [
        "# !pip3 -qq install torch==1.1\n",
        "!pip -qq install nltk==3.2.5\n",
        "!pip -qq install gensim\n",
        "!pip -qq install bokeh==3.2.0\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip -o quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenize and lower-case texts."
      ],
      "metadata": {
        "id": "wgSYdy30O_UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts = texts[:50000] # Accelerated operation\n",
        "print(len(texts))\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in tqdm(texts)]\n",
        "\n",
        "assert len(tokenized_texts) == len(texts)\n",
        "assert isinstance(tokenized_texts[0], list)\n",
        "assert isinstance(tokenized_texts[0][0], str)"
      ],
      "metadata": {
        "id": "t27TcJKJO-ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts[0]"
      ],
      "metadata": {
        "id": "C9K7y4mD2UK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT"
      },
      "source": [
        "2. Collect the indices of the words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PL471pGjuVN"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "\n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. collect the context words\n",
        "\n",
        "First of all, we need to collect all the contexts from our corpus."
      ],
      "metadata": {
        "id": "jHEQOelqQwqP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrsXgaynYPG"
      },
      "source": [
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1)\n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "\n",
        "    return contexts\n",
        "\n",
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_ePiZ7wfpT"
      },
      "source": [
        "Check, what you got:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQNK-9SBdb9"
      },
      "source": [
        "contexts[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l"
      },
      "source": [
        "4. Convert to indices\n",
        "\n",
        "Let's convert words to indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPRlKlLvUBA"
      },
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context])\n",
        "            for central_word, context in contexts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF5mYpCsE9Uh"
      },
      "source": [
        "### **1.2 Skip-Gram Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om1IG5XEMGRa"
      },
      "source": [
        "Word2vec is actually a set of models used to build word embeddings.\n",
        "\n",
        "We are going to start with the *skip-gram model*.\n",
        "\n",
        "It's a very simple neural network with just two layers. It aims to build word vectors that encode information about the co-occurring words:  \n",
        "![](https://i.ibb.co/nL0LLD2/Word2vec-Example.jpg)  \n",
        "\n",
        "More precisely, it models the probabilities $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, where $k$ is the context window size, $c$ is index of the central word (which embedding we are trying to optimize).\n",
        "\n",
        "The learnable parameters of the model are following: matrix $U$ (embeddings' matrix that is used in all downstream tasks. In gensim it's called `syn0`) and matrix $V$ - output layer of the model (in gensim it's called `syn1`).\n",
        "\n",
        "Two vectors correspond to each word: a row in $U$ and a column in $V$. That is $U \\in \\mathbb{R}^{|V|, d}$ and $V \\in \\mathbb{R}^{d, |V|}$, where $d$ is embedding size and $|V|$ is the vocabulary size.\n",
        "\n",
        "As a result, the neural network looks this way:  \n",
        "![skip-gram](https://i.ibb.co/F54XzDC/SkipGram.png)\n",
        "\n",
        "What's going on and how it is connected to probability and word context?\n",
        "\n",
        "Well, the word is mapped to its embedding $u_c$. Then this embedding is multiplied to matrix $V$.\n",
        "\n",
        "As a result, we obtain the set of scores $\\{v_j^T u_c : j \\in {0, \\ldots, |V|}\\}$. Each corresponds to the similarity between the word $w_j$ vector and our word vector. It's very similar to the cosine similarity we calculated in the previous lesson, but without normalization.\n",
        "\n",
        "This similarities show how likely $w_j$ can be in context of word $w_c$. That means, that they can be converted to probability using the softmax function:\n",
        "$$P(w_j | w_c) = \\frac{\\exp(v_{j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)}.$$\n",
        "\n",
        "So for each word we calculate such probability distribution over our vocabulary. It's shown in using blue bars in the picture above. More likely word - bluer is the corresponding cell.\n",
        "\n",
        "The model learns to distribute the probabilities between the co-occuring words for the given one. We'll use cross-entropy loss for it:\n",
        "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
        "\n",
        "For instance, for the sample from the picture model will be penalized if it outputs a low probability of word `over`.\n",
        "\n",
        "Please, notice that we calculate the similarity between vectors from different vector spaces. $u_c$ is the vector from the input embeddings and $v_j$ is the vector from the output embeddings. A high similarity between them means that they co-occur frequently, not that they are similar in the syntactic role or their semantics.\n",
        "\n",
        "On the other hand, the similarity between $u_k$ and $u_m$ means that their output distributions are similar. And that means exactly that the similarity of the count vectors we discussed before and also most probably means their syntactic or semantic similarity.\n",
        "\n",
        "Check this demo to understand what's going on in more depth: [https://ronxin.github.io/wevi/](https://ronxin.github.io/wevi/).\n",
        "\n",
        "Let's implement it now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmrAi9gyIe-"
      },
      "source": [
        "1. **Batches Generations**\n",
        "\n",
        "Neural networks are optimized using stochastic gradient descent methods. This requires a batch generator, a function that produces samples for optimizing the neural network.\n",
        "\n",
        "**Implementation of a Batch Generator:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC9SifFU5iQP"
      },
      "source": [
        "import random\n",
        "\n",
        "def make_skip_gram_batches_iter(contexts, window_size, num_skips, batch_size):\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * window_size\n",
        "\n",
        "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "\n",
        "    batch_size = int(batch_size / num_skips)\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "        batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "        batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "        batch_data, batch_labels = [], []\n",
        "\n",
        "        for data_ind in batch_indices:\n",
        "            central_word, context = central_words[data_ind], contexts[data_ind]\n",
        "\n",
        "            words_to_use = random.sample(context, num_skips)\n",
        "            batch_data.extend([central_word] * num_skips)\n",
        "            batch_labels.extend(words_to_use)\n",
        "        yield {\n",
        "            'tokens': torch.LongTensor(batch_data),\n",
        "            'labels': torch.LongTensor(batch_labels)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmRGZ-vG5iQR"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_Yeowx15iQS"
      },
      "source": [
        "batch = next(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=2, batch_size=32))\n",
        "\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DXjZS3JyQZh"
      },
      "source": [
        "2. **Model**\n",
        "\n",
        "Here is the model implementation of skip-gram model, which has only two layers of neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRw9Z4G__46O"
      },
      "source": [
        "class Skip_Gram_Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs)\n",
        "        out = self.out_layer(embedded)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Training**\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : Train your word embeddngs based on Skip-gram Model."
      ],
      "metadata": {
        "id": "E2CD-lMsSIjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the hyperparameters you can adjust\n",
        "embedding_dim = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 4\n",
        "batch_size = 128\n",
        "\n",
        "# Initialization Model\n",
        "model = Skip_Gram_Model(len(word2index),embedding_dim)\n",
        "# Getting model to GPU\n",
        "model.cuda()\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# use Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "loss_every_nsteps = 3000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(epoch_num):\n",
        "  for step, batch in enumerate(make_skip_gram_batches_iter(contexts, window_size=2, num_skips=4, batch_size=batch_size)):\n",
        "      global_step += 1\n",
        "\n",
        "      # Getting data to the GPU.\n",
        "      tokens, labels = batch['tokens'].cuda(), batch['labels'].cuda()\n",
        "\n",
        "      # make forward pass\n",
        "      logits = model(tokens)\n",
        "\n",
        "      # make backward pass\n",
        "      loss = criterion(logits, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      # apply optimizer\n",
        "      optimizer.step()\n",
        "\n",
        "      # zero grads\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if global_step != 0 and global_step % loss_every_nsteps == 0:\n",
        "          print(\"Epoch = {}, Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(ep, step, total_loss / loss_every_nsteps,\n",
        "                                                                      time.time() - start_time))\n",
        "          total_loss = 0\n",
        "          start_time = time.time()"
      ],
      "metadata": {
        "id": "I8HsUspVSHuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fvE2za248_A"
      },
      "source": [
        "**Obtaining word embeddings**\n",
        "\n",
        "Word embeddings are contained within the embeddings layer of the model. We just need to move them from the GPU to the CPU and convert them to a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWsYkNn-Hnl_"
      },
      "source": [
        "embeddings = model.embeddings.weight.data.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtxY2D01RB6"
      },
      "source": [
        "**Implementing a word similarity search algorithm**\n",
        "\n",
        "Let's check how adequate are similarities that the model learnt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhDwuhDSHEDm"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "\n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "\n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'my')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VS1x-mO1WKS"
      },
      "source": [
        "**Visualization of word embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuXv2HxsAecb"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=1)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "\n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='blue', token=words)\n",
        "\n",
        "\n",
        "visualize_embeddings(embeddings, index2word, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r"
      },
      "source": [
        "### **1.3 Continuous Bag of Words (CBoW) Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX"
      },
      "source": [
        "Now, we will explore another popular Word2Vec paradigm called Continuous Bag of Words (CBoW). *CBoW* offers faster processing and slightly better accuracy for common words compared to the *Skip-Gram*, which is more effective with rare words.\n",
        "\n",
        "**CBoW Structure**\n",
        "\n",
        "Below is the CBoW model architecture:\n",
        "\n",
        "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
        "\n",
        "In CBoW, the goal is to predict a target word from its surrounding context, represented by the sum of context vectors.\n",
        "\n",
        "We will leverage our understanding from the *Skip-Gram* model to implement *CBoW*."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Batches Generations**\n",
        "**<font color=\"red\">[Task]</font>** : Implement the batch generator.\n",
        "\n",
        "**Hint**: The generator should produce a input matrix `(batch_size, 2 * window_size)` containing context word indices and a target matrix `(batch_size)` with central word indices."
      ],
      "metadata": {
        "id": "5ENsl-sbox1m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNaP0uaU7T2-"
      },
      "source": [
        "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
        "\n",
        "    central_words = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    contexts = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "\n",
        "\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "      batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "      batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "\n",
        "      # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBF7xiik7ZaN"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVrQl8S4L9j"
      },
      "source": [
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
        "\n",
        "assert isinstance(batch, dict)\n",
        "assert 'labels' in batch and 'tokens' in batch\n",
        "\n",
        "assert isinstance(batch['tokens'], torch.LongTensor)\n",
        "assert isinstance(batch['labels'], torch.LongTensor)\n",
        "\n",
        "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
        "assert batch['labels'].shape == (batch_size,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKbZ_4E7T3U"
      },
      "source": [
        "2. **Model**\n",
        "**<font color=\"red\">[Task]</font>**: Build the `CBoWModel`.\n",
        "\n",
        "**Hint**: You need to implement the `forward` method based on the CBoW architecture. The context embedding is represented as the average of their context embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkawxwe77T3V"
      },
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "\n",
        "\n",
        "        # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhTDxya7a3S"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh_mNh__6lG2"
      },
      "source": [
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "outputs = model(batch['tokens'].cuda())\n",
        "\n",
        "assert isinstance(outputs, torch.cuda.FloatTensor)\n",
        "assert outputs.shape == (batch_size, len(word2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmn56yki7T3a"
      },
      "source": [
        "3. **Training**\n",
        "**<font color=\"red\">[Task]</font>** : Train the CBoW.\n",
        "\n",
        "**Hint**: Consider referring to the training code of the previously mentioned *Skip-gram* model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKLP9bs7T3b"
      },
      "source": [
        "# Here are the hyperparameters you can adjust\n",
        "embedding_dim = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 4\n",
        "batch_size = 128\n",
        "\n",
        "# Initialization Model\n",
        "model = CBoWModel(len(word2index),embedding_dim)\n",
        "# Getting model to GPU\n",
        "model.cuda()\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# use Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_every_nsteps = 3000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(epoch_num):\n",
        "  for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=batch_size)):\n",
        "      global_step += 1\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "\n",
        "      # ------------------\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if global_step != 0 and global_step % loss_every_nsteps == 0:\n",
        "          print(\"Epoch = {}, Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(ep, step, total_loss / loss_every_nsteps,\n",
        "                                                                      time.time() - start_time))\n",
        "          total_loss = 0\n",
        "          start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Trained Word Embeddings**"
      ],
      "metadata": {
        "id": "X-GyEW7s76-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.embeddings.weight.data.cpu().numpy()\n",
        "most_similar(embeddings, index2word, word2index, 'my')"
      ],
      "metadata": {
        "id": "2DBSdX1MtW7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW4PBdF96xC"
      },
      "source": [
        "**Visualization of our embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTEWcyYmvips"
      },
      "source": [
        "visualize_embeddings(embeddings, index2word, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G2X-TTpzwz"
      },
      "source": [
        "## Task 2: Utilize Word Embeddings\n",
        "\n",
        "Guess, you've seen such pictures already:  \n",
        "\n",
        "![Embeddings Relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*Source: [Tensorflow tutorial on Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "In the first image, we observe the intricate relationships encoded within the word embeddings space. This encompasses various dimensions like gender differences (male-female) or verb tenses.\n",
        "\n",
        "**Interactive Exploration**\n",
        "\n",
        "To delve deeper and interactively explore these relationships, check out these resources:\n",
        "- [Word Vector Demo](http://bionlp-www.utu.fi/wv_demo/)\n",
        "- [Word2Viz](https://lamyiowce.github.io/word2viz/)\n",
        "\n",
        "These tools offer a playful yet insightful experience, allowing you to grasp the nuances and capabilities of word embeddings.\n",
        "\n",
        "**Our task point**\n",
        "\n",
        "Our focus will be on utilizing [gensim](https://radimrehurek.com/gensim/), a well-regarded Python library for word embeddings. Gensim makes it effortless to work with and leverage the power of word embeddings in various applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Use Pretrained Embeddings**\n",
        "Base on gensim, we can easily use a well-pretrained embeddings model. There are a number of such models in <font color=\"blue\">gensim</font>, you can call `api.info()` to get the list."
      ],
      "metadata": {
        "id": "KIvBhh71WIeS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEazfh1s9eki"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use word embedidngs with gensim**\n",
        "\n",
        "Yay, we have loaded well-built word embedings models, now let's learn how to use it.\n",
        "\n",
        "1. To get word's vector, well, call `get_vector`:"
      ],
      "metadata": {
        "id": "HPQxqjIGZxt_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uF6iF6A9uGQ"
      },
      "source": [
        "model.get_vector('anything')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. To get most similar words for the given one :"
      ],
      "metadata": {
        "id": "DiXwAZTsaHCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('bread')"
      ],
      "metadata": {
        "id": "57uH83XZaI6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Analogies with word embeddings\n",
        "\n",
        "It can do such magic (`woman` + `grandfather` - `man`) :\n"
      ],
      "metadata": {
        "id": "Rtyp__uQaVcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "model.most_similar(positive=['woman', 'grandfather'], negative=['man'])"
      ],
      "metadata": {
        "id": "9igEyCm6aqfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this too:"
      ],
      "metadata": {
        "id": "BwCkJSNraruT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model.get_vector('coder') - model.get_vector('brain') + model.get_vector('money')])"
      ],
      "metadata": {
        "id": "e4t94HZXa1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is, who is like coder, with money and without brains."
      ],
      "metadata": {
        "id": "c23cwWhKvtXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** : Run an interesting analogy example\n",
        "\n",
        "**Hint**: Similar to (`woman` + `grandfather` - `man`)"
      ],
      "metadata": {
        "id": "4a15dwaha36x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "g0D04rXsa_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Finding the Most Similar Sentence**\n",
        "\n",
        "In this section, we present a method for sentence retrieval based on word embeddings.\n",
        "\n",
        "The key point is to construct *sentence embeddings*. The simplest method to obtain a sentence embedding is by averaging the embeddings of the words within the sentence.\n",
        "\n",
        "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
        "\n"
      ],
      "metadata": {
        "id": "szS69OEObDzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get Sentence Embedding\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : Implement a function to compute sentence embeddings.\n",
        "\n",
        "**Hint**: Tokenize and lowercase the texts. Calculate the mean embedding for words with known embeddings."
      ],
      "metadata": {
        "id": "FPJdCsjtfDxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embedding(model, sentence):\n",
        "    \"\"\" Calcs sentence embedding as a mean of known word embeddings in the sentence.\n",
        "    If all the words are unknown, returns zero vector.\n",
        "    :param model: KeyedVectors instance\n",
        "    :param sentence: str or list of str (tokenized text)\n",
        "    \"\"\"\n",
        "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        words = word_tokenize(sentence.lower())\n",
        "    else:\n",
        "        words = sentence\n",
        "\n",
        "    sum_embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "    words_in_model = 0\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "metadata": {
        "id": "0LPw1fRg1GaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it:"
      ],
      "metadata": {
        "id": "1FbZbKPI1OZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = get_sentence_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "assert vector.shape == (model.vector_size,)"
      ],
      "metadata": {
        "id": "u5phpDHEcdK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Building the Index**\n",
        "\n",
        "With our method ready, we can now embed all sentences in our corpus for retrieval purposes. In this case, we use data from Quora, sampling 1000 entries randomly, and converting them into sentence embeddings."
      ],
      "metadata": {
        "id": "u-8wclbCdTfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "corpus = list(quora_data.sample(1000)[['question1']].question1.replace(np.nan, '', regex=True).unique())\n",
        "text_vectors = np.array([get_sentence_embedding(model, sentence) for sentence in corpus])"
      ],
      "metadata": {
        "id": "P6B2c-bJdCrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NXrJ-R8leSWR",
        "outputId": "0cd193ff-ff9b-4778-fb19-ef75c30d6abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What's the typical cost of labor for installing a glass wall?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Search**\n",
        "\n",
        "Now we are able perform search of the nearest neighbours to the given sentences in our base!\n",
        "\n",
        "\n",
        "We'll use cosine similarity of two vectors:\n",
        "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
        "\n",
        "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the sentence vectors.*\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : IImplement the following function.\n",
        "\n",
        "**Hint:** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
      ],
      "metadata": {
        "id": "QiMtj5dXebbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    query_vec = get_sentence_embedding(model, query)\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "metadata": {
        "id": "ynEJW6E7eg0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it!"
      ],
      "metadata": {
        "id": "is8SoYmHkQo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_nearest(model, text_vectors, corpus, \"What's your biggest regret in life?\", k=10)"
      ],
      "metadata": {
        "id": "49s4zB1OjXd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bias of Word Embeddings**\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n"
      ],
      "metadata": {
        "id": "qA5CwGV8jU5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example showing word embeddings biases on gender:"
      ],
      "metadata": {
        "id": "pdIUrmnJxGvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "print(model.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ],
      "metadata": {
        "id": "gWngJZCWxduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** Identify an example of bias.\n",
        "\n",
        "**Hint:** Consider providing an example from perspectives such as race or sexual orientation."
      ],
      "metadata": {
        "id": "zpFDY9BByDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "np_RQalnx0Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** Thinking About Bias.\n",
        "\n",
        "**Hint:** Briefly explain how bias can be introduced into word embeddings and suggest one method to mitigate these biases."
      ],
      "metadata": {
        "id": "902RJydFyjMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">Write your answer here.</font>**\n",
        "\n"
      ],
      "metadata": {
        "id": "P8zWOHg3065B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5"
      },
      "source": [
        "## Supplementary Materials\n",
        "Source from [DeepNLP-Course of DanAnastasyev](https://colab.research.google.com/drive/1o65wrq6RYgWyyMvNP8r9ZknXBniDoXrn#forceEdit=true&offline=true&sandboxMode=true)\n",
        "\n",
        "## To read\n",
        "### Blogs\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "\n",
        "### Papers\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Enhancing Embeddings\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Sentence Embeddings\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## To watch\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Acknowledgement\n",
        "\n",
        "This assignment was developed with reference to the following course materials:\n",
        "- [DeepNLP Course by Dan Anastasyev](https://github.com/DanAnastasyev/DeepNLP-Course?tab=readme-ov-file)\n",
        "- [Exploring Word Vectors from Stanford's CS224N](https://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html)\n",
        "- [Natural Language Processing course from Princeton University](https://nlp.cs.princeton.edu/cos484-sp21/)\n",
        "- [Yandex Data School NLP Course Week 1 Seminar](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week01_embeddings/seminar.ipynb#scrollTo=9m7GZWVk-jrW)\n"
      ],
      "metadata": {
        "id": "CJ_O-O0wkool"
      }
    }
  ]
}